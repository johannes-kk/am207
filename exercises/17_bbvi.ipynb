{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 17-bbvi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johannes-kk/am207/blob/master/exercises/17_bbvi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWdYQ3mjNoXS"
      },
      "source": [
        "# DAY 17: Black-box Variational Inference\n",
        "\n",
        "\n",
        "### AM207: Advanced Scientific Computing\n",
        "\n",
        "#### Instructor: Weiwei Pan\n",
        "\n",
        "#### Due: October 29th, 11:59pm EST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x27kWg2dNo7s"
      },
      "source": [
        "**Names of Group Members**:\n",
        "\n",
        "Matthieu Meeus (matthieu_meeus@g.harvard.edu) \n",
        " Maggie Wang (maggiewang@college.harvard.edu)\n",
        "Nari Johnson njohnson@college.harvard.edu\n",
        "Will Seaton (wseaton@g.harvard.edu)\n",
        "Johannes Kolberg (johanneskolberg@g.harvard.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATiLknorOb-l"
      },
      "source": [
        "## Learning Goals:\n",
        "\n",
        "1. Implement BBVI and BBVI with the reparametrization trick\n",
        "2. Compare the variance of score-function and reparametrization gradient esetimators\n",
        "3. Understand the potential drawbacks of BBVI with the mean-field assumption\n",
        "\n",
        "\n",
        "### Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aG7tFRPh2v6"
      },
      "source": [
        "from autograd import numpy as np\n",
        "from autograd import scipy as sp\n",
        "from autograd import grad\n",
        "from autograd.misc.optimizers import adam, sgd\n",
        "\n",
        "# Add elementwise for vectorized inputs\n",
        "from autograd import elementwise_grad as egrad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsnZZVydvQT0"
      },
      "source": [
        "def black_box_variational_inference(logprob, D, num_samples):\n",
        "    \n",
        "    \"\"\"\n",
        "    Implements http://arxiv.org/abs/1401.0118, and uses the\n",
        "    local reparameterization trick from http://arxiv.org/abs/1506.02557\n",
        "    code taken from:\n",
        "    https://github.com/HIPS/autograd/blob/master/examples/black_box_svi.py\n",
        "    \"\"\"\n",
        "\n",
        "    def unpack_params(params):\n",
        "        # Variational dist is a diagonal Gaussian.\n",
        "        mean, log_std = params[:D], params[D:]\n",
        "        return np.array(mean), np.array(log_std)\n",
        "\n",
        "    def gaussian_entropy(log_std):\n",
        "        return 0.5 * D * (1.0 + np.log(2*np.pi)) + np.sum(log_std)\n",
        "\n",
        "    rs = npr.RandomState(0)\n",
        "    def variational_objective(params, t):\n",
        "        \"\"\"Provides a stochastic estimate of the variational lower bound.\"\"\"\n",
        "        mean, log_std = unpack_params(params)\n",
        "        samples = rs.randn(num_samples, D) * np.exp(log_std) + mean\n",
        "        lower_bound = gaussian_entropy(log_std) + np.mean(logprob(samples, t))\n",
        "        return -lower_bound\n",
        "\n",
        "    gradient = grad(variational_objective)\n",
        "\n",
        "    return variational_objective, gradient, unpack_params\n",
        "\n",
        "def variational_inference(Sigma_W, sigma_y, y_train, x_train, forward, S, max_iteration, step_size, verbose):\n",
        "    '''implements wrapper for variational inference via bbb for bayesian regression'''\n",
        "    D = Sigma_W.shape[0]\n",
        "    Sigma_W_inv = np.linalg.inv(Sigma_W)\n",
        "    Sigma_W_det = np.linalg.det(Sigma_W)\n",
        "    variational_dim = D\n",
        "    \n",
        "    #define the log prior on the model parameters\n",
        "    def log_prior(W):\n",
        "        constant_W = -0.5 * (D * np.log(2 * np.pi) + np.log(Sigma_W_det))\n",
        "        exponential_W = -0.5 * np.diag(np.dot(np.dot(W, Sigma_W_inv), W.T))\n",
        "        log_p_W = constant_W + exponential_W\n",
        "        return log_p_W\n",
        "\n",
        "    #define the log likelihood\n",
        "    def log_lklhd(W):\n",
        "        # S = W.shape[0]\n",
        "        constant = (-np.log(sigma_y) - 0.5 * np.log(2 * np.pi)) * N\n",
        "        exponential = -0.5 * sigma_y**-2 * np.sum((y_train.reshape((1, 1, N)) - forward(W, x_train))**2, axis=2).flatten()\n",
        "        return constant + exponential\n",
        "\n",
        "    #define the log joint density\n",
        "    log_density = lambda w, t: log_lklhd(w) + log_prior(w)\n",
        "\n",
        "    #build variational objective.\n",
        "    objective, gradient, unpack_params = black_box_variational_inference(log_density, D, num_samples=S)\n",
        "\n",
        "    def callback(params, t, g):\n",
        "        if verbose:\n",
        "            if  t % 100 == 0:\n",
        "                print(\"Iteration {} lower bound {}; gradient mag: {}\".format(t, -objective(params, t), np.linalg.norm(gradient(params, t))))\n",
        "\n",
        "    print(\"Optimizing variational parameters...\")\n",
        "    #initialize variational parameters\n",
        "    init_mean = np.ones(D)\n",
        "    init_log_std = -100 * np.ones(D)\n",
        "    init_var_params = np.concatenate([init_mean, init_log_std])\n",
        "    \n",
        "    #perform gradient descent using adam (a type of gradient-based optimizer)\n",
        "    variational_params = adam(gradient, init_var_params, step_size=step_size, num_iters=max_iteration, callback=callback)\n",
        "    \n",
        "    return variational_params "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPFwep8g0Tok"
      },
      "source": [
        "## Problem 1: Implementing BBVI\n",
        "In this problem we implement BBVI and BBVI with the reparametrization trick.\n",
        "\n",
        "For the following Bayesian model\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{W}&\\sim p(\\mathbf{W})\\\\\n",
        "Y^{(n)} &\\sim p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W}^s),\n",
        "\\end{align}\n",
        "\n",
        "the ***Black-box Variational Inference (BBVI)*** with the mean-field assumption algorithm approximates the posterior $p(\\mathbf{W} | \\mathrm{Data})$ with a mean-field (i.e. diagonal) Gaussian $q(\\mathbf{W}) = \\mathcal{N}(\\mathbf{W}; \\mu, \\Sigma)$ by minimizing the KL-divergence between the approxaimte posterior and the true posterior:\n",
        "0. **Initialization:** pick an intial value $\\mu^{(0)}, \\Sigma^{(0)}$\n",
        "1. **Gradient Ascent:** repeat:\n",
        "\n",
        "   1. Approximate the gradient \n",
        "   \\begin{align}\n",
        "   \\nabla_{\\mu, \\Sigma} \\, ELBO(\\mathbf{W}) &= \\mathbb{E}_{\\mathbf{W} \\sim q(\\mathbf{W} | \\mu, \\Sigma)}\\left[ \\nabla_{\\mu, \\Sigma}\\, q(\\mathbf{W} | \\mu, \\Sigma) * \\log \\left( \\frac{p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W})}{q(\\mathbf{W} | \\mu, \\Sigma)} \\right) \\right]\\\\\n",
        "   &\\approx\\frac{1}{S}\\underbrace{\\sum_{s=1}^S \\nabla_{\\mu, \\Sigma}\\, \\log q(\\mathbf{W}^s | \\mu, \\Sigma) * \\log \\left( \\frac{p(\\mathbf{W}^s) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W}^s)}{q(\\mathbf{W}^s | \\mu, \\Sigma)} \\right)}_{{\\text{score function gradient}}},\n",
        "   \\end{align}\n",
        "   where $\\mathbf{W}^s\\sim q(\\mathbf{W} | \\mu^{\\text{current}}, \\Sigma^{\\text{current}})$.\n",
        "   2. Update parameters $(\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) \\leftarrow (\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) + \\eta * {\\text{score function gradient}}$\n",
        "\n",
        "We've noted in lecture that this gradient estimate can have high variance and thus proposed a modified version of BBVI: The ***Black-box Variational Inference (BBVI) with the reparametrization trick***. \n",
        "\n",
        "BBVI with the reparametrization trick is as follows:\n",
        "0. **Initialization:** pick an intial value $\\mu^{(0)}, \\Sigma^{(0)}$\n",
        "1. **Gradient Ascent:** repeat:\n",
        "\n",
        "   1. Approximate the gradient \n",
        "   \\begin{align}\n",
        "   \\nabla_{\\mu, \\Sigma} \\, ELBO(\\mathbf{W}) \\approx& \\frac{1}{S} \\sum_{s=1}^S \\nabla_{\\mu, \\Sigma} \\log \\left[p(\\epsilon_s^\\top \\Sigma^{1/2} + \\mu) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\epsilon_s^\\top \\Sigma^{1/2} + \\mu)\\right] \\\\\n",
        "   &- \\nabla_{\\mu, \\Sigma}\\underbrace{\\mathbb{E}_{\\mathbf{W} \\sim \\mathcal{N}(\\mu, \\Sigma )}\\left[\\log \\mathcal{N}(\\mathbf{W};\\mu, \\Sigma ) \\right]}_{\\text{Guassian entropy: has closed form}},\n",
        "   \\end{align}\n",
        "   where $\\epsilon_s \\sim \\mathcal{N}(0, \\mathbf{I})$.\n",
        "   2. Update parameters $(\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) \\leftarrow (\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) + \\eta * {\\text{reparametrization gradient}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu8X8NiXp0a-",
        "outputId": "af89b255-36af-4471-b8b8-2cb16f677ce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "mu_init = 1.\n",
        "sigma_init = 1.\n",
        "num_iteration = 10\n",
        "\n",
        "mu_curr = mu_init\n",
        "sigma_curr = sigma_init\n",
        "S = 1\n",
        "\n",
        "num_iterations = 50\n",
        "\n",
        "gaussian_log_pdf = lambda mu, sigma_sq, W: -0.5 * (np.log(2 * np.pi * sigma_sq) + (x - mu)**2 / sigma_sq) \n",
        "\n",
        "def gaussian_pdf(mean, sd, x):\n",
        "    var = float(sd)**2\n",
        "    denom = (2*math.pi*var)**.5\n",
        "    num = math.exp(-(float(x)-float(mean))**2/(2*var))\n",
        "    return num/denom\n",
        "\n",
        "def log_posterior_num():\n",
        "  return np.log(0.5 * gaussian_pdf(-1., -0.5, W) + 0.8 * gaussian_pdf(4., 2., W))\n",
        "\n",
        "def score_function_grad(mu_q, sigma_q):\n",
        "  log_q = gaussian_log_pdf(mu_q, sigma_q, W)\n",
        "  second_term = log_posterior_num(W) - log_q\n",
        "  grad_obj = log_q * second_term\n",
        "  return grad_obj\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  # Sample S ws from q(W)\n",
        "  W = np.random.normal(loc=mu_curr, scale=sigma_curr, size=S)\n",
        "\n",
        "  mu_grad = grad(score_function_grad)(mu_q)\n",
        "  sigma_grad = grad(score_function_grad)(sigma_q)\n",
        "\n",
        "  mu_new = mu_curr + eta * np.mean(mu_grad)\n",
        "  sigma_new = sigma_curr + eta * np.mean(sigma_grad)\n",
        "\n",
        "  np.mean(sigma_grad)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bc163dbcf330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mmu_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_function_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0msigma_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_function_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mu_q' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq6MEh8y-Mzu",
        "outputId": "af6a7c0a-f2cb-4f49-f0b6-70faa41ae52d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# NEW VERSION Johannes\n",
        "\n",
        "mu_init = 1.\n",
        "sigma_init = 2.\n",
        "\n",
        "# Learning rate\n",
        "eta = 1.0\n",
        "# Dimension of weights (wouldn't this just be number of predictorws?)\n",
        "num_weights = 1\n",
        "# Samples per iteration (right?)\n",
        "S = 3\n",
        "# Iterations of gradient descent\n",
        "num_iterations = 50\n",
        "\n",
        "gaussian_log_pdf = lambda mu, sigma_sq, W: -0.5 * (np.log(2 * np.pi * sigma_sq) + (W - mu)**2 / sigma_sq) \n",
        "\n",
        "def gaussian_pdf(mean, sd, w):\n",
        "    var = float(sd)**2\n",
        "    denom = (2*np.pi*var)**.5\n",
        "    num = np.exp(-(w-float(mean))**2/(2*var))\n",
        "    return num/denom\n",
        "\n",
        "def log_posterior_num(W):\n",
        "  return np.log(0.5 * gaussian_pdf(-1., -0.5, W) + 0.8 * gaussian_pdf(4., 2., W))\n",
        "\n",
        "def score_function_grad(mu_q, sigma_q, W):\n",
        "  log_q = gaussian_log_pdf(mu_q, sigma_q, W)\n",
        "  second_term = log_posterior_num(W) - log_q\n",
        "  grad_obj = log_q * second_term\n",
        "  return grad_obj\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "elbo_grad_mu = 0\n",
        "elbo_grad_sigma = 0\n",
        "\n",
        "elbo_grad = 0\n",
        "\n",
        "mu_new = mu_init\n",
        "sigma_new = sigma_init\n",
        "for i in range(num_iterations):\n",
        "  print('Iteration', i)\n",
        "  for s in range(S):\n",
        "    #print('   Sample', s)\n",
        "    # Sample S ws from q(W)\n",
        "    W = np.random.normal(loc=mu_new, scale=sigma_new, size=num_weights)\n",
        "\n",
        "    elbo_grad += egrad(score_function_grad)(mu_new, sigma_new, W)\n",
        "    #elbo_grad_mu += egrad(score_function_grad)(mu_new)\n",
        "    #elbo_grad_sigma += egrad(score_function_grad)(sigma_new)\n",
        "\n",
        "  elbo_grad /= num_iterations\n",
        "  #elbo_grad_mu /= num_iterations\n",
        "  #elbo_grad_sigma /= num_iterations\n",
        "\n",
        "  mu_new += eta*elbo_grad\n",
        "  sigma_new += eta*elbo_grad\n",
        "  print(' ', mu_new, sigma_new)\n",
        "  #mu_new = mu_curr + eta * np.mean(mu_grad)\n",
        "  #sigma_new = sigma_curr + eta * np.mean(sigma_grad)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0\n",
            "  1.0236031897525026 2.0236031897525026\n",
            "Iteration 1\n",
            "  1.194023138803256 2.194023138803256\n",
            "Iteration 2\n",
            "  1.4215570719121544 2.4215570719121544\n",
            "Iteration 3\n",
            "  1.4396607713347005 2.4396607713347005\n",
            "Iteration 4\n",
            "  1.1986010461720564 2.1986010461720564\n",
            "Iteration 5\n",
            "  1.1066373738593434 2.1066373738593436\n",
            "Iteration 6\n",
            "  1.0746461488360513 2.0746461488360515\n",
            "Iteration 7\n",
            "  0.9527159150496989 1.952715915049699\n",
            "Iteration 8\n",
            "  0.8485054248310347 1.848505424831035\n",
            "Iteration 9\n",
            "  0.8358869563690503 1.8358869563690505\n",
            "Iteration 10\n",
            "  1.0766400772405142 2.0766400772405142\n",
            "Iteration 11\n",
            "  0.9238948828176237 1.9238948828176237\n",
            "Iteration 12\n",
            "  0.6969543379323011 1.696954337932301\n",
            "Iteration 13\n",
            "  0.7023109002928137 1.7023109002928136\n",
            "Iteration 14\n",
            "  0.5929428337153924 1.5929428337153924\n",
            "Iteration 15\n",
            "  0.5997460602779957 1.5997460602779956\n",
            "Iteration 16\n",
            "  0.4735198181404672 1.473519818140467\n",
            "Iteration 17\n",
            "  0.4497944686995407 1.4497944686995405\n",
            "Iteration 18\n",
            "  0.45963489832567844 1.4596348983256784\n",
            "Iteration 19\n",
            "  0.4808488000112907 1.4808488000112907\n",
            "Iteration 20\n",
            "  0.4100168203888135 1.4100168203888135\n",
            "Iteration 21\n",
            "  0.41226358578567396 1.412263585785674\n",
            "Iteration 22\n",
            "  0.4327567040083683 1.4327567040083684\n",
            "Iteration 23\n",
            "  0.518818424061513 1.518818424061513\n",
            "Iteration 24\n",
            "  0.441537681285053 1.441537681285053\n",
            "Iteration 25\n",
            "  0.4503634468397462 1.4503634468397464\n",
            "Iteration 26\n",
            "  0.3303181872738189 1.330318187273819\n",
            "Iteration 27\n",
            "  0.3973298953768292 1.3973298953768294\n",
            "Iteration 28\n",
            "  0.3733801130226716 1.3733801130226717\n",
            "Iteration 29\n",
            "  0.35460761986747114 1.3546076198674712\n",
            "Iteration 30\n",
            "  0.34697310517567476 1.3469731051756748\n",
            "Iteration 31\n",
            "  0.24754274786223396 1.247542747862234\n",
            "Iteration 32\n",
            "  0.23360590322439628 1.2336059032243964\n",
            "Iteration 33\n",
            "  0.1437283362205991 1.1437283362205992\n",
            "Iteration 34\n",
            "  0.11475428265695059 1.1147542826569508\n",
            "Iteration 35\n",
            "  0.23993125226297127 1.2399312522629715\n",
            "Iteration 36\n",
            "  0.11716462931260976 1.11716462931261\n",
            "Iteration 37\n",
            "  0.4455533145800339 1.445553314580034\n",
            "Iteration 38\n",
            "  0.4512847751620756 1.4512847751620759\n",
            "Iteration 39\n",
            "  0.42566793338186437 1.4256679333818647\n",
            "Iteration 40\n",
            "  0.46874191690306555 1.4687419169030658\n",
            "Iteration 41\n",
            "  0.6963953015181334 1.6963953015181337\n",
            "Iteration 42\n",
            "  0.6260242165901283 1.6260242165901284\n",
            "Iteration 43\n",
            "  0.5009102494380995 1.5009102494380995\n",
            "Iteration 44\n",
            "  0.3846257546340968 1.3846257546340968\n",
            "Iteration 45\n",
            "  0.45750853190453955 1.4575085319045396\n",
            "Iteration 46\n",
            "  0.38478135205192265 1.3847813520519228\n",
            "Iteration 47\n",
            "  0.3291778837501109 1.329177883750111\n",
            "Iteration 48\n",
            "  0.25115869177786515 1.2511586917778652\n",
            "Iteration 49\n",
            "  0.157237898906073 1.157237898906073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgWpV5uCE8JF"
      },
      "source": [
        "\n",
        "**Exercise 1:** Let's set the unnormalized true posterior distribution to be:\n",
        "\\begin{align}\n",
        "p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W}) = 0.5 \\mathcal{N}(\\mathbf{W}; -1, 0.5) + 0.8 \\mathcal{N}(\\mathbf{W}; 4, 2)\n",
        "\\end{align}\n",
        "where $\\mathbf{W} \\in \\mathbb{R}$.\n",
        "\n",
        "Implement the score-function gradient estimator as well as the reparametrization gradient estimator. Which estimator has higher variance? Try $S = 1, 10, 30, 50, 100$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mFz98rapo8V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnBdeA03pwQ7"
      },
      "source": [
        "**Exercise 2:** Implement BBVI with the reparametrization trick (see sample code in notes for Lecture #17, you'll need to modify the `variational_inference` function and pass the above unnormalized posterior when we call `black_box_variational_inference`, instead of passing it the `logdensity` that is constructed in the current version of `variational_inference`). Apply it to approximate the above unnormalized posterior distribution. Plot both the unnormalized posterior and the optimal approximate posterior. How well does your BBVI solution capture the true unnormalized posterior? Is this expected?\n",
        "\n",
        "You might want to change the form of the unnormalized posterior (try adding more mixture components to the mixture model) and see how well our BBVI does. \n",
        "\n",
        "Based on these experiments, what are the potential draw backs of our realization of BBVI? What can we do to improve our posterior approximation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPXSq2Cvi2w9"
      },
      "source": [
        "def generate_data(N, y_var=1.):\n",
        "    '''generate training data with a gap, testing data uniformly sampled'''\n",
        "\n",
        "\n",
        "    # Sample from the posterior p(w | x, y)\n",
        "    # Use this sample of w and our forward to compute y from x_train\n",
        "\n",
        "    #training x\n",
        "    x_train = np.hstack((np.linspace(-10, -5, N), np.linspace(5, 10, N)))\n",
        "    #function relating x and y\n",
        "#     f = lambda x:  0.01 * x**3\n",
        "\n",
        "    f = lambda x: 2 * x\n",
        "    #y is equal to f(x) plus gaussian noise\n",
        "    y_train = f(x_train) + np.random.normal(0, y_var**0.5, 2 * N)\n",
        "\n",
        "    ## generate testing data\n",
        "    #nubmer of testing points\n",
        "    N_test = 100\n",
        "    #testing x\n",
        "    x_test = np.linspace(-10, 10, N_test)\n",
        "    y_test = f(x_test) + np.random.normal(0, y_var**0.5, N_test)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDvIHagHm0lG"
      },
      "source": [
        "def bayesian_polynomial_regression(x, y, x_test, y_test, prior_var, y_var, ax, S=100, poly_degree=10):\n",
        "    '''visualize the posterior predictive of a Bayesian polynomial regression model'''\n",
        "    poly = PolynomialFeatures(poly_degree)\n",
        "    #transform training x: add polynomial features\n",
        "    x_poly = poly.fit_transform(x.reshape((-1, 1)))\n",
        "    #transform x_test: add polynomial features\n",
        "    x_test_poly = poly.fit_transform(x_test.reshape((-1, 1)))\n",
        "    #Gaussian log pdf\n",
        "    gaussian_log_pdf = lambda mu, sigma_sq, x: -0.5 * (np.log(2 * np.pi * sigma_sq) + (x - mu)**2 / sigma_sq)\n",
        "\n",
        "    #reshape y into 2D array\n",
        "    y_matrix = y.reshape((-1, 1))\n",
        "\n",
        "    #define the covariance and precision matrices of the prior on the weights\n",
        "    prior_variance = np.diag(prior_var * np.ones((x_poly.shape[1], )))\n",
        "    prior_precision = np.linalg.inv(prior_variance)\n",
        "\n",
        "    #defining the posterior variance\n",
        "    joint_variance = np.linalg.inv(prior_precision + 1. / y_var * x_poly.T.dot(x_poly))\n",
        "    #defining the posterior mean\n",
        "    joint_mean = joint_variance.dot(x_poly.T.dot(y_matrix)) * 1. / y_var\n",
        "\n",
        "    #sampling S points from the posterior\n",
        "    posterior_samples = np.random.multivariate_normal(joint_mean.flatten(), joint_variance, size=S)\n",
        "    #sampling S points from the posterior predictive\n",
        "    y_predict_noiseless = np.array([x_test_poly.dot(sample) for sample in posterior_samples])\n",
        "    y_predict_bayes = y_predict_noiseless + np.random.normal(0, y_var**0.5, size=(S, len(x_test)))\n",
        "    \n",
        "    #compute log likelihood for the test data\n",
        "    log_likelihood_bayes = []\n",
        "    for n in range(len(y_test)):\n",
        "        log_likelihood_bayes.append(gaussian_log_pdf(y_predict_noiseless[:, n], y_var, y_test[n]).mean())\n",
        "    log_likelihood_bayes = np.array(log_likelihood_bayes)\n",
        "    \n",
        "    #compute the 95 percentiles of the posterior predictives\n",
        "    ub_bayes = np.percentile(y_predict_bayes, 97.5, axis=0)\n",
        "    lb_bayes = np.percentile(y_predict_bayes, 2.5, axis=0)\n",
        "    \n",
        "    #visualize the posterior predictive distribution\n",
        "    ax.scatter(x, y, color='red', s=10, alpha=0.5, label='train data')\n",
        "    ax.fill_between(x_test, ub_bayes, lb_bayes, color='blue', alpha=0.2, label=\"test log-likelihood: {}\".format(np.round(np.sum(log_likelihood_bayes), 4)))\n",
        "    ax.set_title('posterior predictive distribution of bayesian regression model with prior variance of {}'.format(prior_var))\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlim([-10, 10])\n",
        "    \n",
        "    return ax, joint_variance, joint_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_B3YNVjkNON"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import autograd.numpy.random as npr\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQVkNmRTjodc"
      },
      "source": [
        "##set random seed\n",
        "rand_state = 0\n",
        "random = np.random.RandomState(rand_state)\n",
        "\n",
        "##generate training data\n",
        "#number of points in each of the two segments of the domain\n",
        "N = 20\n",
        "#output variance\n",
        "y_var = 1.\n",
        "x_train, y_train, x_test, y_test = generate_data(N, y_var)\n",
        "\n",
        "##transform covariates for polynomial regression\n",
        "poly = PolynomialFeatures(1)\n",
        "#transform x: add polynomial features\n",
        "x_poly = poly.fit_transform(x_train.reshape((-1, 1)))\n",
        "#transform x_test: add polynomial features\n",
        "x_test_poly = poly.fit_transform(x_test.reshape((-1, 1)))\n",
        "\n",
        "##define dimensions\n",
        "N = x_poly.shape[0]\n",
        "D = x_poly.shape[1]\n",
        "\n",
        "##define variances\n",
        "sigma_y = 1.**2\n",
        "weight_noise = 5**2\n",
        "Sigma_W = weight_noise * np.eye(D)\n",
        "\n",
        "##polynomial function\n",
        "def forward(w, x):\n",
        "    x_poly = poly.fit_transform(x.reshape((-1, 1))) \n",
        "    return np.dot(w, x_poly.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgOaqw5_j6MI"
      },
      "source": [
        "#S = 1\n",
        "S_arr = [1, 10, 30, 50, 100]\n",
        "max_iteration = 2000\n",
        "step_size = 1e-2\n",
        "verbose=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp_Z5BMvf6l6",
        "outputId": "d5216e27-f955-43de-8762-cbcb9afb86a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        }
      },
      "source": [
        "for S in S_arr:\n",
        "  S_param_estimates = []\n",
        "  for j in range(1):\n",
        "    S_param_estimates.append(variational_inference(Sigma_W, sigma_y, y_train, x_train, forward, S, max_iteration, step_size, verbose))\n",
        "  \n",
        "  print(S_param_estimates)\n",
        "  S_var = np.var(S_param_estimates)\n",
        "  print(S_var)\n",
        "  print('Variance for ' + str(S) + ' is ' + str(S_var))\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing variational parameters...\n",
            "[array([-5.38335290e-03,  1.99173220e+00, -8.00000002e+01, -8.00000002e+01])]\n",
            "1640.472142698992\n",
            "Variance for 1 is 1640.472142698992\n",
            "\n",
            "\n",
            "Optimizing variational parameters...\n",
            "[array([-5.38335290e-03,  1.99173220e+00, -8.00000002e+01, -8.00000002e+01])]\n",
            "1640.472142698992\n",
            "Variance for 10 is 1640.472142698992\n",
            "\n",
            "\n",
            "Optimizing variational parameters...\n",
            "[array([-5.38335290e-03,  1.99173220e+00, -8.00000002e+01, -8.00000002e+01])]\n",
            "1640.472142698992\n",
            "Variance for 30 is 1640.472142698992\n",
            "\n",
            "\n",
            "Optimizing variational parameters...\n",
            "[array([-5.38335290e-03,  1.99173220e+00, -8.00000002e+01, -8.00000002e+01])]\n",
            "1640.472142698992\n",
            "Variance for 50 is 1640.472142698992\n",
            "\n",
            "\n",
            "Optimizing variational parameters...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-25495e9c7145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mS_param_estimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mS_param_estimates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariational_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSigma_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_param_estimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-3c9037072735>\u001b[0m in \u001b[0;36mvariational_inference\u001b[0;34m(Sigma_W, sigma_y, y_train, x_train, forward, S, max_iteration, step_size, verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m#perform gradient descent using adam (a type of gradient-based optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mvariational_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_var_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvariational_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/misc/optimizers.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(grad, x0, callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/misc/optimizers.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(grad, x, callback, num_iters, step_size, b1, b2, eps)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m      \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m  \u001b[0;31m# First  moment estimate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/misc/optimizers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, i)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_x0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0m_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/differential_operators.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         raise TypeError(\"Grad only applies to real scalar-output functions. \"\n\u001b[1;32m     28\u001b[0m                         \"Try jacobian, elementwise_grad or holomorphic_grad.\")\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0munary_to_nary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/core.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/core.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(g, end_node)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoposort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mingrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moutgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_outgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \"VJP of {} wrt argnum 0 not defined\".format(fun.__name__))\n\u001b[1;32m     66\u001b[0m             \u001b[0mvjp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjpfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0margnum_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnum_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/numpy/numpy_vjps.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munbroadcast_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0mtarget_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munbroadcast_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/numpy/numpy_vjps.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     35\u001b[0m                         lambda ans, x, y : unbroadcast_f(y, lambda g: x * g))\n\u001b[1;32m     36\u001b[0m defvjp(anp.subtract,    lambda ans, x, y : unbroadcast_f(x, lambda g: g),\n\u001b[0;32m---> 37\u001b[0;31m                         lambda ans, x, y : unbroadcast_f(y, lambda g: -g))\n\u001b[0m\u001b[1;32m     38\u001b[0m defvjp(anp.divide,      lambda ans, x, y : unbroadcast_f(x, lambda g:   g / y),\n\u001b[1;32m     39\u001b[0m                         lambda ans, x, y : unbroadcast_f(y, lambda g: - g * x / y**2))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RoYV4G9ngoj",
        "outputId": "13fe5dd3-30be-4c4a-977f-add8e0df91a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "variational_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -0.1357079 ,   2.01506752, -80.0000002 , -80.0000002 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xojntGgFt3FA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}